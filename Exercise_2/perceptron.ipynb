{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "by\n",
    "Henrik Albers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "output_and = np.array(\n",
    "    # AND\n",
    "    [0, 0, 0, 1]\n",
    "    )\n",
    "output_or = np.array(\n",
    "    # OR\n",
    "    [0, 1, 1, 1]\n",
    "    )\n",
    "output_nor = np.array(\n",
    "    # NOR\n",
    "    [1, 0, 0, 0]\n",
    "    )\n",
    "output_xor = np.array(\n",
    "    # XOR\n",
    "    [0, 1, 1, 0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Implement single-layer neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "class Perceptron:\n",
    "    def __init__(self) -> None:\n",
    "        self.weights = np.random.rand(2)\n",
    "        self.bias = np.random.rand(1)\n",
    "    \n",
    "    def activation_function(self, input):\n",
    "        if input[0] >= 0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def feed_fwd(self, x):\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation_function(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Implement backpropagation using the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(X, Y, learning_rate, epochs):\n",
    "    perceptron = Perceptron()\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in zip(X, Y):\n",
    "            prediction = perceptron.feed_fwd(x)\n",
    "            # Update weights and bias using the Perceptron learning rule\n",
    "            error = y - prediction\n",
    "\n",
    "            # Calculate the gradients\n",
    "            gradient_weights = -2 * error * x\n",
    "            gradient_bias = -2 * error\n",
    "            # Update weights and bias using gradient descent\n",
    "            perceptron.weights -= learning_rate * gradient_weights\n",
    "            perceptron.bias -= learning_rate * gradient_bias\n",
    "    return perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train and test the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Perceptron for AND\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "and_perceptron = train_perceptron(input, output_and, learning_rate, epochs)\n",
    "\n",
    "# Training the Perceptron for OR\n",
    "or_perceptron = train_perceptron(input, output_or, learning_rate, epochs)\n",
    "\n",
    "# Training the Perceptron for NOR\n",
    "nor_perceptron = train_perceptron(input, output_nor, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND([0 0]) = 0\n",
      "OR([0 0]) = 0\n",
      "NOR([0 0]) = 1\n",
      "AND([0 1]) = 0\n",
      "OR([0 1]) = 1\n",
      "NOR([0 1]) = 0\n",
      "AND([1 0]) = 0\n",
      "OR([1 0]) = 1\n",
      "NOR([1 0]) = 0\n",
      "AND([1 1]) = 1\n",
      "OR([1 1]) = 1\n",
      "NOR([1 1]) = 0\n"
     ]
    }
   ],
   "source": [
    "for x in input:\n",
    "    print(f\"AND({x}) = {and_perceptron.feed_fwd(x)}\")\n",
    "    print(f\"OR({x}) = {or_perceptron.feed_fwd(x)}\")\n",
    "    print(f\"NOR({x}) = {nor_perceptron.feed_fwd(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Can a single-layer perceptron represent the XOR Boolean function?\n",
    "No the XOR Boolean function can not be represented by a single-layer perceptron, because the results of it can not be separated by a linear line. To be able to represent a the XOR function a hidden layer needs to be added, to allow for non-linear separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Enhance the neural network to accurately represent the XOR Boolean function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xor_Perceptron:\n",
    "    def __init__(self, and_perceptron, or_perceptron) -> None:\n",
    "        self.and_perceptron = and_perceptron\n",
    "        self.or_perceptron = or_perceptron\n",
    "    \n",
    "    def train_perceptron_xor(self, X, Y, learning_rate, epochs):\n",
    "        perceptron = Perceptron()\n",
    "        for epoch in range(epochs):\n",
    "            for x, y in zip(X, Y):\n",
    "                # reusing trained AND and OR functions as building blocks\n",
    "                or_res = self.or_perceptron.feed_fwd(x)\n",
    "                and_res = self.and_perceptron.feed_fwd(x)\n",
    "                \n",
    "                # Updating w&b of hidden layer\n",
    "                prediction = perceptron.feed_fwd([and_res, or_res])\n",
    "                # Update weights and bias using the Perceptron learning rule\n",
    "                error = y - prediction\n",
    "                # Calculate the gradients\n",
    "                gradient_weights = -2 * error \n",
    "                gradient_weights *= np.array([and_res, or_res]) # * x\n",
    "                gradient_bias = -2 * error\n",
    "                # Update weights and bias using gradient descent\n",
    "                perceptron.weights -= learning_rate * gradient_weights\n",
    "                perceptron.bias -= learning_rate * gradient_bias\n",
    "        self.perceptron = perceptron\n",
    "        \n",
    "    def predict_xor(self, X):\n",
    "        or_res = self.or_perceptron.feed_fwd(X)\n",
    "        and_res = self.and_perceptron.feed_fwd(X)\n",
    "        xor_res = self.perceptron.feed_fwd([and_res, or_res])\n",
    "        return xor_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Perceptron for XOR\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "xor_perceptron = Xor_Perceptron(and_perceptron, or_perceptron)\n",
    "xor_perceptron.train_perceptron_xor(input, output_xor, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR([0 0]) = 0\n",
      "XOR([0 1]) = 1\n",
      "XOR([1 0]) = 1\n",
      "XOR([1 1]) = 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in input:\n",
    "    print(f\"XOR({x}) = {xor_perceptron.predict_xor(x)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
